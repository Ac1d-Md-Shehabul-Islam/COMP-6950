{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89d41bac",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f87a1764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671c9640",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "499e62d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class readdata():\n",
    "    \n",
    "    def __init__(self,pathname):\n",
    "        self.pathname = pathname\n",
    "    \n",
    "    def getDataFrame(self):\n",
    "        df = pd.read_csv(self.pathname)\n",
    "        return df\n",
    "    \n",
    "    def dropColumn(self, df):\n",
    "        df.drop(['reviewerID','asin','reviewerName','helpful/0','helpful/1','summary','unixReviewTime','reviewTime'], axis=1, inplace=True)\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85986c7d",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec6dadf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocess():\n",
    "    import string\n",
    "    import nltk\n",
    "#     nltk.download('stopwords')\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    \n",
    "    def __init__(self,dataframe):\n",
    "        self.df = dataframe\n",
    "        \n",
    "    def rename(self):\n",
    "        df.rename(columns={'reviewText':'feedback', 'overall':'rating'}, inplace=True)\n",
    "        return df\n",
    "    \n",
    "    def addLabel(self):\n",
    "        labels = []\n",
    "\n",
    "        for i in df['rating']:\n",
    "            if i<=2:\n",
    "                labels.append('Negative')\n",
    "            elif i==3:\n",
    "                labels.append('Neutral')\n",
    "            else:\n",
    "                labels.append('Positive')\n",
    "        output = pd.DataFrame(labels)\n",
    "        df['label'] = output\n",
    "        return df\n",
    "    \n",
    "    def getTextLen(self): # shows the length of words excluding whitespaces in a message body.\n",
    "        df['text_length'] = df['feedback'].apply(len) \n",
    "        return df.head(4)\n",
    "    \n",
    "    def punc_count(self, txt):\n",
    "        import string\n",
    "        string.punctuation\n",
    "        \n",
    "        count = sum([1 for c in txt if c in string.punctuation])\n",
    "        return (count/len(txt))*100\n",
    "    \n",
    "    def getPuncPerc(self):      \n",
    "        df['punc_%'] = df['feedback'].apply(lambda x: self.punc_count(x))\n",
    "        return df\n",
    "    \n",
    "    def remove_punc(self, txt):\n",
    "        import string\n",
    "        txt_nopunc = ''.join([i for i in txt if i not in string.punctuation])\n",
    "        return txt_nopunc\n",
    "    \n",
    "    def getCleanText(self):\n",
    "        df['clean_text'] = df['feedback'].apply(self.remove_punc) \n",
    "        return df\n",
    "        \n",
    "    def tokenize(self,txt):\n",
    "        import re\n",
    "        tokens = re.split('\\W+',txt) # W means non-word characters and + means one or more\n",
    "        return tokens\n",
    "    \n",
    "    def tokenizedCleanText(self):\n",
    "        df['tokenized_clean_text'] = df['clean_text'].apply(lambda x: self.tokenize(x.lower()))\n",
    "        return df\n",
    "\n",
    "    def remove_stopwords(self,txt):\n",
    "        import nltk\n",
    "#         nltk.download('stopwords')\n",
    "        stopwords = nltk.corpus.stopwords.words('english')\n",
    "#         print(stopwords[0:10])\n",
    "        \n",
    "        clean_msg = [word for word in txt if word not in stopwords]\n",
    "        return clean_msg\n",
    "    \n",
    "    def textNoStopWord(self):\n",
    "        df['text_no_stopword'] = df['tokenized_clean_text'].apply(self.remove_stopwords)\n",
    "        return df\n",
    "    \n",
    "    def stemming(self, txt): # PorterSemmer is a popular one\n",
    "        import nltk\n",
    "        from nltk.stem import PorterStemmer\n",
    "        ps = PorterStemmer()\n",
    "        \n",
    "        text = [ps.stem(word) for word in txt]\n",
    "        return text\n",
    "    \n",
    "    def ps_stem(self):\n",
    "        df['ps_stem'] = df['text_no_stopword'].apply(self.stemming)\n",
    "        df\n",
    "        \n",
    "    def lemmatization(self, txt):\n",
    "        import nltk\n",
    "        from nltk.stem import WordNetLemmatizer\n",
    "        wn = nltk.WordNetLemmatizer()\n",
    "#         nltk.download('wordnet')\n",
    "        \n",
    "        text = [wn.lemmatize(word) for word in txt]\n",
    "        return text\n",
    "    \n",
    "    def wn_lemm(self):\n",
    "        df['wn_lemmatize'] = df['text_no_stopword'].apply(self.lemmatization)\n",
    "        return df\n",
    "    \n",
    "    def processText(self):\n",
    "        import re\n",
    "        import nltk\n",
    "        from nltk.corpus import stopwords\n",
    "\n",
    "        lemmatizer = nltk.WordNetLemmatizer()\n",
    "        words = stopwords.words('english')\n",
    "        df['processed_text'] = df['clean_text'].apply(lambda x: \" \".join([lemmatizer.lemmatize(i) for i in re.sub(\"[^a-zA-Z]\", \" \", x).split() if i not in words]).lower())\n",
    "        df\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c369ff6",
   "metadata": {},
   "source": [
    "### Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f896b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class view():\n",
    "\n",
    "    def __init__(self,dataframe):\n",
    "        self.df = dataframe\n",
    "      \n",
    "    def plot_rating(self):\n",
    "        plt.hist(df['label'], color = 'lightblue', edgecolor = 'black')\n",
    "        plt.title('Output')\n",
    "        plt.xlabel('Reviews')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_label(self):\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.pie(df['label'].value_counts(), labels=[\"Positive\", \"Negative\",'Neutral'], autopct='%.1f%%',colors = ['lightblue','orange'])\n",
    "        plt.title(\"Output Label Distribution\")\n",
    "        plt.show() \n",
    "        \n",
    "    def wordCompare(self): # We can clearly see that Positive have a high number of words as compared to Negatives. So itâ€™s a good feature to distinguish.\n",
    "        bins = np.linspace(0, 200, 40)\n",
    "\n",
    "        plt.hist(df[df['label']=='Positive']['text_length'], bins, alpha=0.5, label='Positive')\n",
    "        plt.hist(df[df['label']=='Negative']['text_length'], bins, alpha=0.5, label='Negative')\n",
    "        plt.xlabel('text_length')\n",
    "        plt.ylabel('frequency')\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.show()\n",
    "       \n",
    "    def puncCompare(self): # Positive has a percentage of punctuations but not that far away from Negative. Surprising as at times Positive feedbacks can contain a lot of punctuation marks. But still, it can be identified as a good feature.\n",
    "        bins = np.linspace(0, 50, 50)\n",
    "\n",
    "        plt.hist(df[df['label']=='Positive']['punc_%'], bins, alpha=0.5, label='Positive')\n",
    "        plt.hist(df[df['label']=='Negative']['punc_%'], bins, alpha=0.5, label='Negative')\n",
    "        plt.xlabel('punc_percentage')\n",
    "        plt.ylabel('frequency')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba11121",
   "metadata": {},
   "source": [
    "### Train-Test Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b6ab70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class train_test():\n",
    "    \n",
    "    def __init__(self,dataframe):\n",
    "        self.df = dataframe  \n",
    "        self.X = self.df['processed_text']\n",
    "        self.y = self.df['label']\n",
    "        self.X_train = []\n",
    "        self.y_train = []\n",
    "        self.X_test = []\n",
    "        self.y_test = []\n",
    "        \n",
    "    def split(self):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(self.df['processed_text'], self.y, test_size = 0.3, random_state=100)\n",
    "        print(df.shape); print(X_train.shape); print(X_test.shape)\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        \n",
    "        return y_train, y_test\n",
    "\n",
    "    def tfidf_vec(self):\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "        vectorizer_tfidf = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "        train_tfIdf = vectorizer_tfidf.fit_transform(self.X_train.values.astype('U'))\n",
    "        test_tfIdf = vectorizer_tfidf.transform(self.X_test.values.astype('U'))\n",
    "\n",
    "#         print(vectorizer_tfidf.get_feature_names()[:10])\n",
    "        return train_tfIdf, test_tfIdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4794721",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85aecdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model():\n",
    "    \n",
    "    def __init__(self,dataframe):\n",
    "        self.df = dataframe  \n",
    "    \n",
    "    def naive_bayes(self, a, b, c, d):\n",
    "        import pandas as pd\n",
    "        from sklearn.naive_bayes import MultinomialNB\n",
    "        from sklearn import metrics\n",
    "\n",
    "        nb_classifier = MultinomialNB()\n",
    "\n",
    "#         nb_classifier.fit(train_tfIdf, y_train)\n",
    "        nb_classifier.fit(a, c)\n",
    "\n",
    "#         predNB = nb_classifier.predict(test_tfIdf) \n",
    "        predNB = nb_classifier.predict(b)\n",
    "        print(predNB[:10])\n",
    "\n",
    "#         Conf_metrics_tfidf = metrics.confusion_matrix(y_test, predNB, labels=['Positive', 'Neutral', 'Negative'])\n",
    "        Conf_metrics_tfidf = metrics.confusion_matrix(d, predNB, labels=['Positive', 'Neutral', 'Negative'])\n",
    "        print(Conf_metrics_tfidf)\n",
    "        \n",
    "        from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "#         print(confusion_matrix(y_test,predNB))\n",
    "        print(confusion_matrix(d,predNB))\n",
    "#         print(classification_report(d,predNB))\n",
    "        a = classification_report(d,predNB, output_dict=True)\n",
    "        df = pd.DataFrame(a)\n",
    "        df.to_csv('naive_bayes.csv', index=False)\n",
    "        \n",
    "        \n",
    "    def random_forest(self, a, b, c, d):\n",
    "        import pandas as pd\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        from sklearn import metrics\n",
    "        from sklearn.metrics import confusion_matrix\n",
    "\n",
    "        rf_classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 100)\n",
    "\n",
    "#         rf_classifier.fit(train_tfIdf, y_train)\n",
    "        rf_classifier.fit(a, c)\n",
    "        \n",
    "#         predRF = rf_classifier.predict(test_tfIdf) \n",
    "        predRF = rf_classifier.predict(b)\n",
    "        print(predRF[:-2])\n",
    "\n",
    "        # Calculate the accuracy score\n",
    "#         accuracy_RF = metrics.accuracy_score(y_test, predRF)\n",
    "        accuracy_RF = metrics.accuracy_score(d, predRF)\n",
    "        print(f'Accuracy: {accuracy_RF*100} %')\n",
    "\n",
    "#         Conf_metrics_RF = metrics.confusion_matrix(y_test, predRF, labels=['Positive', 'Neutral', 'Negative'])\n",
    "        Conf_metrics_RF = metrics.confusion_matrix(d, predRF, labels=['Positive', 'Neutral', 'Negative'])\n",
    "        print(Conf_metrics_RF)\n",
    "        \n",
    "        from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "#         print(confusion_matrix(y_test,predRF))\n",
    "#         print(classification_report(y_test,predRF))\n",
    "        print(confusion_matrix(d,predRF))\n",
    "        a = classification_report(d,predRF, output_dict=True)\n",
    "        df = pd.DataFrame(a)\n",
    "        df.to_csv('random_forest.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
